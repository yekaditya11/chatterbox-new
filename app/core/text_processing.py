"""
Text processing utilities for TTS
"""

import gc
import torch
import re
from typing import List, Optional, Tuple
from app.config import Config
from app.models.long_text import LongTextChunk


def split_text_into_chunks(text: str, max_length: int = None) -> list:
    """Split text into manageable chunks for TTS processing"""
    if max_length is None:
        max_length = Config.MAX_CHUNK_LENGTH
    
    if len(text) <= max_length:
        return [text]
    
    # Try to split at sentence boundaries first
    sentence_endings = ['. ', '! ', '? ', '.\n', '!\n', '?\n']
    chunks = []
    current_chunk = ""
    
    # Split into sentences
    sentences = []
    temp_text = text
    
    while temp_text:
        best_split = len(temp_text)
        best_ending = ""
        
        for ending in sentence_endings:
            pos = temp_text.find(ending)
            if pos != -1 and pos < best_split:
                best_split = pos + len(ending)
                best_ending = ending
        
        if best_split == len(temp_text):
            # No sentence ending found, take the rest
            sentences.append(temp_text)
            break
        else:
            sentences.append(temp_text[:best_split])
            temp_text = temp_text[best_split:]
    
    # Group sentences into chunks
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) <= max_length:
            current_chunk += (" " if current_chunk else "") + sentence
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # If single sentence is too long, split it further
            if len(sentence) > max_length:
                # Split at commas, semicolons, etc.
                sub_delimiters = [', ', '; ', ' - ', ' — ']
                sub_chunks = [sentence]
                
                for delimiter in sub_delimiters:
                    new_sub_chunks = []
                    for chunk in sub_chunks:
                        if len(chunk) <= max_length:
                            new_sub_chunks.append(chunk)
                        else:
                            parts = chunk.split(delimiter)
                            current_part = ""
                            for part in parts:
                                if len(current_part) + len(delimiter) + len(part) <= max_length:
                                    current_part += (delimiter if current_part else "") + part
                                else:
                                    if current_part:
                                        new_sub_chunks.append(current_part)
                                    current_part = part
                            if current_part:
                                new_sub_chunks.append(current_part)
                    sub_chunks = new_sub_chunks
                
                # Add sub-chunks
                for sub_chunk in sub_chunks:
                    if len(sub_chunk) <= max_length:
                        chunks.append(sub_chunk.strip())
                    else:
                        # Last resort: split by words
                        words = sub_chunk.split()
                        current_word_chunk = ""
                        for word in words:
                            if len(current_word_chunk) + len(word) + 1 <= max_length:
                                current_word_chunk += (" " if current_word_chunk else "") + word
                            else:
                                if current_word_chunk:
                                    chunks.append(current_word_chunk)
                                current_word_chunk = word
                        if current_word_chunk:
                            chunks.append(current_word_chunk)
                current_chunk = ""
            else:
                current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    # Filter out empty chunks
    chunks = [chunk for chunk in chunks if chunk.strip()]
    
    return chunks


def split_text_for_streaming(
    text: str,
    chunk_size: Optional[int] = None,
    strategy: Optional[str] = None,
    quality: Optional[str] = None
) -> List[str]:
    """
    Split text into chunks optimized for streaming with different strategies.
    
    Args:
        text: Input text to split
        chunk_size: Target chunk size (characters)
        strategy: Splitting strategy ('sentence', 'paragraph', 'fixed', 'word')
        quality: Quality preset ('fast', 'balanced', 'high')
    
    Returns:
        List of text chunks optimized for streaming
    """
    # Apply quality presets
    if quality:
        if quality == "fast":
            chunk_size = chunk_size or 100
            strategy = strategy or "word"
        elif quality == "balanced":
            chunk_size = chunk_size or 200
            strategy = strategy or "sentence"
        elif quality == "high":
            chunk_size = chunk_size or 300
            strategy = strategy or "paragraph"
    
    # Set defaults
    chunk_size = chunk_size or 200
    strategy = strategy or "sentence"
    
    # Apply strategy-specific splitting
    if strategy == "paragraph":
        return _split_by_paragraphs(text, chunk_size)
    elif strategy == "sentence":
        return _split_by_sentences(text, chunk_size)
    elif strategy == "word":
        return _split_by_words(text, chunk_size)
    elif strategy == "fixed":
        return _split_by_fixed_size(text, chunk_size)
    else:
        # Default to sentence splitting
        return _split_by_sentences(text, chunk_size)


def _split_by_paragraphs(text: str, max_length: int) -> List[str]:
    """Split text by paragraph breaks, respecting max length"""
    # Split by double newlines (paragraph breaks)
    paragraphs = re.split(r'\n\s*\n', text.strip())
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
        
        # If paragraph fits with current chunk
        if len(current_chunk) + len(paragraph) + 2 <= max_length:  # +2 for paragraph break
            if current_chunk:
                current_chunk += "\n\n" + paragraph
            else:
                current_chunk = paragraph
        else:
            # Save current chunk if it exists
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # If paragraph is too long, split it by sentences
            if len(paragraph) > max_length:
                sentence_chunks = _split_by_sentences(paragraph, max_length)
                chunks.extend(sentence_chunks)
                current_chunk = ""
            else:
                current_chunk = paragraph
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return [chunk for chunk in chunks if chunk.strip()]


def _split_by_sentences(text: str, max_length: int) -> List[str]:
    """Split text by sentence boundaries, respecting max length"""
    # Enhanced sentence splitting regex
    sentence_pattern = r'(?<=[.!?])\s+'
    sentences = re.split(sentence_pattern, text.strip())
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # If sentence fits with current chunk
        if len(current_chunk) + len(sentence) + 1 <= max_length:  # +1 for space
            if current_chunk:
                current_chunk += " " + sentence
            else:
                current_chunk = sentence
        else:
            # Save current chunk if it exists
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # If sentence is too long, split it further
            if len(sentence) > max_length:
                sub_chunks = _split_long_sentence(sentence, max_length)
                chunks.extend(sub_chunks)
                current_chunk = ""
            else:
                current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return [chunk for chunk in chunks if chunk.strip()]


def _split_by_words(text: str, max_length: int) -> List[str]:
    """Split text by word boundaries, respecting max length"""
    words = text.split()
    chunks = []
    current_chunk = ""
    
    for word in words:
        # If word fits with current chunk
        if len(current_chunk) + len(word) + 1 <= max_length:  # +1 for space
            if current_chunk:
                current_chunk += " " + word
            else:
                current_chunk = word
        else:
            # Save current chunk if it exists
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # If single word is too long, force it into its own chunk
            if len(word) > max_length:
                # Split very long words at character boundaries
                for i in range(0, len(word), max_length):
                    chunks.append(word[i:i + max_length])
                current_chunk = ""
            else:
                current_chunk = word
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return [chunk for chunk in chunks if chunk.strip()]


def _split_by_fixed_size(text: str, chunk_size: int) -> List[str]:
    """Split text into fixed-size chunks"""
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i + chunk_size].strip()
        if chunk:
            chunks.append(chunk)
    
    return chunks


def _split_long_sentence(sentence: str, max_length: int) -> List[str]:
    """Split a long sentence at natural break points"""
    # Try to split at commas, semicolons, etc.
    delimiters = [', ', '; ', ' - ', ' — ', ': ', ' and ', ' or ', ' but ']
    
    chunks = [sentence]
    
    for delimiter in delimiters:
        new_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_length:
                new_chunks.append(chunk)
            else:
                parts = chunk.split(delimiter)
                current_part = ""
                for part in parts:
                    if len(current_part) + len(delimiter) + len(part) <= max_length:
                        current_part += (delimiter if current_part else "") + part
                    else:
                        if current_part:
                            new_chunks.append(current_part)
                        current_part = part
                if current_part:
                    new_chunks.append(current_part)
        chunks = new_chunks
    
    # Final fallback: split by words
    final_chunks = []
    for chunk in chunks:
        if len(chunk) <= max_length:
            final_chunks.append(chunk)
        else:
            word_chunks = _split_by_words(chunk, max_length)
            final_chunks.extend(word_chunks)
    
    return [chunk.strip() for chunk in final_chunks if chunk.strip()]


def get_streaming_settings(
    streaming_chunk_size: Optional[int],
    streaming_strategy: Optional[str],
    streaming_quality: Optional[str]
) -> dict:
    """
    Get optimized streaming settings based on parameters.
    
    Returns a dictionary with optimized settings for streaming.
    """
    settings = {
        "chunk_size": streaming_chunk_size or 200,
        "strategy": streaming_strategy or "sentence",
        "quality": streaming_quality or "balanced"
    }
    
    # Apply quality presets if not explicitly overridden
    if streaming_quality and not streaming_chunk_size:
        if streaming_quality == "fast":
            settings["chunk_size"] = 100
        elif streaming_quality == "high":
            settings["chunk_size"] = 300
    
    if streaming_quality and not streaming_strategy:
        if streaming_quality == "fast":
            settings["strategy"] = "word"
        elif streaming_quality == "high":
            settings["strategy"] = "paragraph"
    
    return settings


def concatenate_audio_chunks(audio_chunks: list, sample_rate: int) -> torch.Tensor:
    """Concatenate multiple audio tensors with proper memory management"""
    if len(audio_chunks) == 1:
        return audio_chunks[0]
    
    # Add small silence between chunks (0.1 seconds)
    silence_samples = int(0.1 * sample_rate)
    
    # Create silence tensor on the same device as audio chunks
    device = audio_chunks[0].device if hasattr(audio_chunks[0], 'device') else 'cpu'
    silence = torch.zeros(1, silence_samples, device=device)
    
    # Use torch.no_grad() to prevent gradient tracking
    with torch.no_grad():
        concatenated = audio_chunks[0]
        
        for i, chunk in enumerate(audio_chunks[1:], 1):
            # Concatenate current result with silence and next chunk
            concatenated = torch.cat([concatenated, silence, chunk], dim=1)
            
            # Optional: cleanup intermediate tensors for very long sequences
            if i % 10 == 0:  # Every 10 chunks
                gc.collect()
    
    # Clean up silence tensor
    del silence
    
    return concatenated


def split_text_for_long_generation(text: str,
                                   max_chunk_size: Optional[int] = None,
                                   overlap_chars: int = 0) -> List[LongTextChunk]:
    """
    Split long text into chunks optimized for TTS generation with intelligent boundaries.

    This function implements a hierarchical splitting strategy:
    1. First attempt: Split at paragraph boundaries (double newlines)
    2. Second attempt: Split at sentence boundaries (. ! ?)
    3. Third attempt: Split at clause boundaries (, ; : - —)
    4. Last resort: Split at word boundaries

    Args:
        text: Input text to split (should be > 3000 characters)
        max_chunk_size: Maximum characters per chunk (defaults to Config.LONG_TEXT_CHUNK_SIZE)
        overlap_chars: Number of characters to overlap between chunks for context

    Returns:
        List of LongTextChunk objects with metadata
    """
    if max_chunk_size is None:
        max_chunk_size = Config.LONG_TEXT_CHUNK_SIZE

    # Ensure we don't exceed the regular TTS limit
    effective_max = min(max_chunk_size, Config.MAX_TOTAL_LENGTH - 100)  # Leave some buffer

    chunks = []
    chunk_index = 0
    remaining_text = text.strip()

    while remaining_text:
        if len(remaining_text) <= effective_max:
            # Last chunk
            chunk_text = remaining_text
            remaining_text = ""
        else:
            # Find the best split point
            chunk_text, remaining_text = _find_best_split_point(
                remaining_text, effective_max, overlap_chars
            )

        # Create chunk metadata
        chunk = LongTextChunk(
            index=chunk_index,
            text=chunk_text,
            text_preview=chunk_text[:50] + ("..." if len(chunk_text) > 50 else ""),
            character_count=len(chunk_text)
        )

        chunks.append(chunk)
        chunk_index += 1

    return chunks


def _find_best_split_point(text: str, max_length: int, overlap_chars: int = 0) -> Tuple[str, str]:
    """
    Find the best point to split text while preserving semantic boundaries.

    Returns:
        Tuple of (chunk_text, remaining_text)
    """
    if len(text) <= max_length:
        return text, ""

    # Strategy 1: Split at paragraph boundaries
    split_result = _try_split_at_paragraphs(text, max_length, overlap_chars)
    if split_result:
        return split_result

    # Strategy 2: Split at sentence boundaries
    split_result = _try_split_at_sentences(text, max_length, overlap_chars)
    if split_result:
        return split_result

    # Strategy 3: Split at clause boundaries
    split_result = _try_split_at_clauses(text, max_length, overlap_chars)
    if split_result:
        return split_result

    # Strategy 4: Split at word boundaries (last resort)
    return _split_at_words(text, max_length, overlap_chars)


def _try_split_at_paragraphs(text: str, max_length: int, overlap_chars: int) -> Optional[Tuple[str, str]]:
    """Try to split at paragraph boundaries (double newlines)"""
    # Find all paragraph breaks
    paragraph_pattern = r'\n\s*\n'
    matches = list(re.finditer(paragraph_pattern, text))

    if not matches:
        return None

    # Find the best paragraph break within our limit
    best_split = None
    for match in matches:
        split_pos = match.end()
        if split_pos <= max_length:
            best_split = split_pos
        else:
            break

    if best_split and best_split > max_length * 0.5:  # Don't take chunks that are too small
        chunk_text = text[:best_split].strip()
        remaining_text = text[max(0, best_split - overlap_chars):].strip()
        return chunk_text, remaining_text

    return None


def _try_split_at_sentences(text: str, max_length: int, overlap_chars: int) -> Optional[Tuple[str, str]]:
    """Try to split at sentence boundaries"""
    # Enhanced sentence boundary detection
    sentence_endings = ['. ', '! ', '? ', '.\n', '!\n', '?\n', '."', '!"', '?"', ".'", "!'", "?'"]

    best_split = None
    for ending in sentence_endings:
        pos = 0
        while pos < len(text):
            found = text.find(ending, pos)
            if found == -1:
                break

            split_pos = found + len(ending)
            if split_pos <= max_length:
                best_split = split_pos
                pos = found + 1
            else:
                break

    if best_split and best_split > max_length * 0.4:  # Don't take chunks that are too small
        chunk_text = text[:best_split].strip()
        remaining_text = text[max(0, best_split - overlap_chars):].strip()
        return chunk_text, remaining_text

    return None


def _try_split_at_clauses(text: str, max_length: int, overlap_chars: int) -> Optional[Tuple[str, str]]:
    """Try to split at clause boundaries (commas, semicolons, etc.)"""
    clause_delimiters = [', ', '; ', ': ', ' - ', ' — ', ' and ', ' or ', ' but ', ' while ', ' when ']

    best_split = None
    for delimiter in clause_delimiters:
        pos = 0
        while pos < len(text):
            found = text.find(delimiter, pos)
            if found == -1:
                break

            split_pos = found + len(delimiter)
            if split_pos <= max_length:
                best_split = split_pos
                pos = found + 1
            else:
                break

    if best_split and best_split > max_length * 0.3:  # Don't take chunks that are too small
        chunk_text = text[:best_split].strip()
        remaining_text = text[max(0, best_split - overlap_chars):].strip()
        return chunk_text, remaining_text

    return None


def _split_at_words(text: str, max_length: int, overlap_chars: int) -> Tuple[str, str]:
    """Split at word boundaries as last resort"""
    if len(text) <= max_length:
        return text, ""

    # Find the last space before our limit
    split_pos = text.rfind(' ', 0, max_length)

    if split_pos == -1:  # No space found, force split
        split_pos = max_length

    chunk_text = text[:split_pos].strip()
    remaining_text = text[max(0, split_pos - overlap_chars):].strip()

    return chunk_text, remaining_text


def estimate_processing_time(text_length: int, avg_chars_per_second: float = 25.0) -> int:
    """
    Estimate processing time for long text TTS generation.

    Args:
        text_length: Total characters in text
        avg_chars_per_second: Average processing rate (characters per second)

    Returns:
        Estimated processing time in seconds
    """
    # Base estimate + overhead for chunking and concatenation
    base_time = text_length / avg_chars_per_second

    # Add overhead: 5 seconds for setup + 2 seconds per chunk + 10 seconds for concatenation
    num_chunks = max(1, (text_length + Config.LONG_TEXT_CHUNK_SIZE - 1) // Config.LONG_TEXT_CHUNK_SIZE)
    overhead = 5 + (num_chunks * 2) + 10

    return int(base_time + overhead)


def validate_long_text_input(text: str) -> Tuple[bool, str]:
    """
    Validate text for long text TTS generation.

    Returns:
        Tuple of (is_valid, error_message)
    """
    if not text or not text.strip():
        return False, "Input text cannot be empty"

    text_length = len(text.strip())

    if text_length <= Config.MAX_TOTAL_LENGTH:
        return False, f"Text is {text_length} characters. Use regular TTS for texts under {Config.MAX_TOTAL_LENGTH} characters"

    if text_length > Config.LONG_TEXT_MAX_LENGTH:
        return False, f"Text is too long ({text_length} characters). Maximum allowed: {Config.LONG_TEXT_MAX_LENGTH}"

    # Check for excessive repetition (potential spam/abuse)
    words = text.split()
    if len(set(words)) < len(words) * 0.1:  # Less than 10% unique words
        return False, "Text appears to be excessively repetitive"

    return True, "" 